The model that uses 2 hidden states simply splits the input into the class of
vowels and space, and the class of consonants. The most prominent symbol is the
space character. The most prominent consonants are t,n,r,s. These observations
are consistent with the general notion of the structure of english text. The
probabilities of a consonant following a vowel or a vowel following a consonant
are roughly .8 and the the probabilities of two vowels or two consonants in a
row are roughly .2. 

The model with 4 hidden states is a lot less easier to interpret. The model
still distiguishes a class of vowels (column 2 in the output probabilities
diagram). The model appears to designate two classes to consonants (columns 3 
and 4 in the output probabilities diagram). After many trial cases the model
failed to establish a class of liquids that was described in class. Instead
there seems to be a class designated almost entirely to the space
character, column 1(minor probabilities are assigned to letters c,f,i,p,r,u,x).
This state never has a transition to itself (naturally). 

A character generated by class 1 is most likely to be followed by a character 
from class 4, followed by class3.

Characters from class of vowels are most likely to be followed by a character
from class 3. 

Characters from class 3 are most likely to be word ending characters.

Characters from class 4 are very likely to be followed by a vowel.

Overall, it's hard to say why the model settles to these probability
distributions, save for establishing the vowels as a separate class. This might
have to do with the specific nature of the string the model was trained on.
Perhaps if the model was trained on a larger set of data which is more
representative of the english languge the classes would be easier to identify. 
